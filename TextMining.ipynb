{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "e2a7c3d20469dda411fd3211f02092a920078d1ca97c72d3dc7928b841d3a44a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Mining\n",
    "\n",
    "- Vertiente en la ciencia de datos relevante\n",
    "- Calidad de la información\n",
    "- Permite encontrar texto en cualquier parte\n",
    "- Procesa dicha informacion para su aprovechamiento\n",
    "- Un buen analisis permite ser escalabe \n",
    "\n",
    "## Proceso tipico\n",
    "- Obtener la informacion en bruto\n",
    "- Bag of words\n",
    "- Lematización (Reducir sus palabras a palabras clave)\n",
    "- Analisis avanzados como traduccion, etc.\n",
    "- Modelización\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bag of words\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "texto=open(path + \"/ejemplo.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Tesla continÃºa desplegando su porfolio de vehÃ\\xadculos elÃ©ctricos con paso firme.', 'Las primeras unidades del Model Y que han llegado a EspaÃ±a ya estÃ¡n siendo entregadas a sus propietarios, aunque hace ya casi un aÃ±o y medio que este modelo fue lanzado en Estados Unidos.', 'Este SUV compacto y 100% elÃ©ctrico comparte plataforma, mecÃ¡nica, tecnologÃ\\xada, y, en definitiva, el 75% de sus componentes, con el Model 3, que llegÃ³ a nuestras carreteras hace algo mÃ¡s de dos aÃ±os.', 'Hemos tenido la oportunidad de probarlo a fondo durante un dÃ\\xada completo en el que nos hemos enfrentado a un desafÃ\\xado: sacar el mÃ¡ximo partido posible al tiempo que tenÃ\\xadamos para poner a prueba tanto su despliegue tecnolÃ³gico, en el que brilla con mucha intensidad una asistencia a la conducciÃ³n muy avanzada, como la experiencia que nos propone cuando colocamos nuestras manos sobre su volante y pisamos el acelerador.', 'AhÃ\\xad va un pequeÃ±o espÃ³iler: lo hemos conseguido.', 'Y hemos disfrutado haciÃ©ndolo por una razÃ³n: la deportividad que estÃ¡ presente en los otros coches de Tesla tambiÃ©n estÃ¡ codificada en el ADN de este Model Y. Es un SUV, y, por tanto, su envergadura es mayor que la del Model 3 con el que tanto tiene en comÃºn, pero basta experimentar su brutal aceleraciÃ³n y la forma en que nos permite tomar las curvas para darse cuenta de que es un coche con el que es posible divertirse.', 'Mucho.']\n"
     ]
    }
   ],
   "source": [
    "frases = sent_tokenize(texto)\n",
    "print(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Tesla', 'continÃºa', 'desplegando', 'su', 'porfolio', 'de', 'vehÃ\\xadculos', 'elÃ©ctricos', 'con', 'paso', 'firme', '.', 'Las', 'primeras', 'unidades', 'del', 'Model', 'Y', 'que', 'han', 'llegado', 'a', 'EspaÃ±a', 'ya', 'estÃ¡n', 'siendo', 'entregadas', 'a', 'sus', 'propietarios', ',', 'aunque', 'hace', 'ya', 'casi', 'un', 'aÃ±o', 'y', 'medio', 'que', 'este', 'modelo', 'fue', 'lanzado', 'en', 'Estados', 'Unidos', '.', 'Este', 'SUV', 'compacto', 'y', '100', '%', 'elÃ©ctrico', 'comparte', 'plataforma', ',', 'mecÃ¡nica', ',', 'tecnologÃ\\xada', ',', 'y', ',', 'en', 'definitiva', ',', 'el', '75', '%', 'de', 'sus', 'componentes', ',', 'con', 'el', 'Model', '3', ',', 'que', 'llegÃ³', 'a', 'nuestras', 'carreteras', 'hace', 'algo', 'mÃ¡s', 'de', 'dos', 'aÃ±os', '.', 'Hemos', 'tenido', 'la', 'oportunidad', 'de', 'probarlo', 'a', 'fondo', 'durante', 'un', 'dÃ\\xada', 'completo', 'en', 'el', 'que', 'nos', 'hemos', 'enfrentado', 'a', 'un', 'desafÃ\\xado', ':', 'sacar', 'el', 'mÃ¡ximo', 'partido', 'posible', 'al', 'tiempo', 'que', 'tenÃ\\xadamos', 'para', 'poner', 'a', 'prueba', 'tanto', 'su', 'despliegue', 'tecnolÃ³gico', ',', 'en', 'el', 'que', 'brilla', 'con', 'mucha', 'intensidad', 'una', 'asistencia', 'a', 'la', 'conducciÃ³n', 'muy', 'avanzada', ',', 'como', 'la', 'experiencia', 'que', 'nos', 'propone', 'cuando', 'colocamos', 'nuestras', 'manos', 'sobre', 'su', 'volante', 'y', 'pisamos', 'el', 'acelerador', '.', 'AhÃ\\xad', 'va', 'un', 'pequeÃ±o', 'espÃ³iler', ':', 'lo', 'hemos', 'conseguido', '.', 'Y', 'hemos', 'disfrutado', 'haciÃ©ndolo', 'por', 'una', 'razÃ³n', ':', 'la', 'deportividad', 'que', 'estÃ¡', 'presente', 'en', 'los', 'otros', 'coches', 'de', 'Tesla', 'tambiÃ©n', 'estÃ¡', 'codificada', 'en', 'el', 'ADN', 'de', 'este', 'Model', 'Y.', 'Es', 'un', 'SUV', ',', 'y', ',', 'por', 'tanto', ',', 'su', 'envergadura', 'es', 'mayor', 'que', 'la', 'del', 'Model', '3', 'con', 'el', 'que', 'tanto', 'tiene', 'en', 'comÃºn', ',', 'pero', 'basta', 'experimentar', 'su', 'brutal', 'aceleraciÃ³n', 'y', 'la', 'forma', 'en', 'que', 'nos', 'permite', 'tomar', 'las', 'curvas', 'para', 'darse', 'cuenta', 'de', 'que', 'es', 'un', 'coche', 'con', 'el', 'que', 'es', 'posible', 'divertirse', '.', 'Mucho', '.']\n"
     ]
    }
   ],
   "source": [
    "palabras = word_tokenize(texto)\n",
    "print(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<FreqDist with 152 samples and 262 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(',', 14),\n",
       " ('que', 13),\n",
       " ('el', 9),\n",
       " ('en', 8),\n",
       " ('de', 7),\n",
       " ('.', 7),\n",
       " ('a', 7),\n",
       " ('un', 6),\n",
       " ('y', 6),\n",
       " ('la', 6),\n",
       " ('su', 5),\n",
       " ('con', 5),\n",
       " ('Model', 4),\n",
       " ('nos', 3),\n",
       " ('hemos', 3),\n",
       " (':', 3),\n",
       " ('tanto', 3),\n",
       " ('es', 3),\n",
       " ('Tesla', 2),\n",
       " ('del', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "stop_word = stopwords.words(\"spanish\")\n",
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras2 = [x for x in palabras if x not in stop_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras2 = word_tokenize(texto.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras2 = [x for x in palabras if x not in stop_word + list(string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Tesla',\n",
       " 'continÃºa',\n",
       " 'desplegando',\n",
       " 'porfolio',\n",
       " 'vehÃ\\xadculos',\n",
       " 'elÃ©ctricos',\n",
       " 'paso',\n",
       " 'firme',\n",
       " 'Las',\n",
       " 'primeras',\n",
       " 'unidades',\n",
       " 'Model',\n",
       " 'Y',\n",
       " 'llegado',\n",
       " 'EspaÃ±a',\n",
       " 'estÃ¡n',\n",
       " 'siendo',\n",
       " 'entregadas',\n",
       " 'propietarios',\n",
       " 'aunque',\n",
       " 'hace',\n",
       " 'casi',\n",
       " 'aÃ±o',\n",
       " 'medio',\n",
       " 'modelo',\n",
       " 'lanzado',\n",
       " 'Estados',\n",
       " 'Unidos',\n",
       " 'Este',\n",
       " 'SUV',\n",
       " 'compacto',\n",
       " '100',\n",
       " 'elÃ©ctrico',\n",
       " 'comparte',\n",
       " 'plataforma',\n",
       " 'mecÃ¡nica',\n",
       " 'tecnologÃ\\xada',\n",
       " 'definitiva',\n",
       " '75',\n",
       " 'componentes',\n",
       " 'Model',\n",
       " '3',\n",
       " 'llegÃ³',\n",
       " 'carreteras',\n",
       " 'hace',\n",
       " 'mÃ¡s',\n",
       " 'dos',\n",
       " 'aÃ±os',\n",
       " 'Hemos',\n",
       " 'oportunidad',\n",
       " 'probarlo',\n",
       " 'fondo',\n",
       " 'dÃ\\xada',\n",
       " 'completo',\n",
       " 'enfrentado',\n",
       " 'desafÃ\\xado',\n",
       " 'sacar',\n",
       " 'mÃ¡ximo',\n",
       " 'partido',\n",
       " 'posible',\n",
       " 'tiempo',\n",
       " 'tenÃ\\xadamos',\n",
       " 'poner',\n",
       " 'prueba',\n",
       " 'despliegue',\n",
       " 'tecnolÃ³gico',\n",
       " 'brilla',\n",
       " 'mucha',\n",
       " 'intensidad',\n",
       " 'asistencia',\n",
       " 'conducciÃ³n',\n",
       " 'avanzada',\n",
       " 'experiencia',\n",
       " 'propone',\n",
       " 'colocamos',\n",
       " 'manos',\n",
       " 'volante',\n",
       " 'pisamos',\n",
       " 'acelerador',\n",
       " 'AhÃ\\xad',\n",
       " 'va',\n",
       " 'pequeÃ±o',\n",
       " 'espÃ³iler',\n",
       " 'conseguido',\n",
       " 'Y',\n",
       " 'disfrutado',\n",
       " 'haciÃ©ndolo',\n",
       " 'razÃ³n',\n",
       " 'deportividad',\n",
       " 'estÃ¡',\n",
       " 'presente',\n",
       " 'coches',\n",
       " 'Tesla',\n",
       " 'tambiÃ©n',\n",
       " 'estÃ¡',\n",
       " 'codificada',\n",
       " 'ADN',\n",
       " 'Model',\n",
       " 'Y.',\n",
       " 'Es',\n",
       " 'SUV',\n",
       " 'envergadura',\n",
       " 'mayor',\n",
       " 'Model',\n",
       " '3',\n",
       " 'comÃºn',\n",
       " 'basta',\n",
       " 'experimentar',\n",
       " 'brutal',\n",
       " 'aceleraciÃ³n',\n",
       " 'forma',\n",
       " 'permite',\n",
       " 'tomar',\n",
       " 'curvas',\n",
       " 'darse',\n",
       " 'cuenta',\n",
       " 'coche',\n",
       " 'posible',\n",
       " 'divertirse',\n",
       " 'Mucho']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "palabras2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Model', 4),\n",
       " ('Tesla', 2),\n",
       " ('Y', 2),\n",
       " ('hace', 2),\n",
       " ('SUV', 2),\n",
       " ('3', 2),\n",
       " ('posible', 2),\n",
       " ('estÃ¡', 2),\n",
       " ('continÃºa', 1),\n",
       " ('desplegando', 1),\n",
       " ('porfolio', 1),\n",
       " ('vehÃ\\xadculos', 1),\n",
       " ('elÃ©ctricos', 1),\n",
       " ('paso', 1),\n",
       " ('firme', 1),\n",
       " ('Las', 1),\n",
       " ('primeras', 1),\n",
       " ('unidades', 1),\n",
       " ('llegado', 1),\n",
       " ('EspaÃ±a', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "FreqDist(palabras2).most_common(20)"
   ]
  },
  {
   "source": [
    "## Stemming: procesamiento textual\n",
    "- Ya sabemos como eliminar las palabras que menos interesan de un texto y emplear unicamente las que nos interesan\n",
    "- Un stemmer es reducir las palabras a la raiz, como llevar los verbos a su raiz"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "texto=open(path + \"/ejemplo.txt\", \"r\").read()\n",
    "stop_words = stopwords.words(\"spanish\")\n",
    "\n",
    "palabras = [x for x in word_tokenize(texto.lower()) if x not in stop_words + list(string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raices = []\n",
    "for palabra in palabras:\n",
    "    raices.append(stemmer.stem(palabra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('model', 5),\n",
       " ('tesl', 2),\n",
       " ('elã©ctric', 2),\n",
       " ('hac', 2),\n",
       " ('suv', 2),\n",
       " ('3', 2),\n",
       " ('posibl', 2),\n",
       " ('estã¡', 2),\n",
       " ('coch', 2),\n",
       " ('continãº', 1),\n",
       " ('despleg', 1),\n",
       " ('porfoli', 1),\n",
       " ('vehã\\xadcul', 1),\n",
       " ('pas', 1),\n",
       " ('firm', 1),\n",
       " ('primer', 1),\n",
       " ('unidad', 1),\n",
       " ('lleg', 1),\n",
       " ('espaã±', 1),\n",
       " ('estã¡n', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "FreqDist(raices).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('model', 4),\n",
       " ('tesla', 2),\n",
       " ('hace', 2),\n",
       " ('suv', 2),\n",
       " ('3', 2),\n",
       " ('posible', 2),\n",
       " ('estã¡', 2),\n",
       " ('continãºa', 1),\n",
       " ('desplegando', 1),\n",
       " ('porfolio', 1),\n",
       " ('vehã\\xadculos', 1),\n",
       " ('elã©ctricos', 1),\n",
       " ('paso', 1),\n",
       " ('firme', 1),\n",
       " ('primeras', 1),\n",
       " ('unidades', 1),\n",
       " ('llegado', 1),\n",
       " ('espaã±a', 1),\n",
       " ('estã¡n', 1),\n",
       " ('siendo', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "FreqDist(palabras).most_common(20)"
   ]
  },
  {
   "source": [
    "## Analisis posicional del texto\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\alber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "texto=open(path + \"/english_example.txt\", \"r\").read()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "palabras = [x for x in word_tokenize(texto.lower()) if x not in stop_words + list(string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run\nrun\n"
     ]
    }
   ],
   "source": [
    "#english|\\\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "temmer = SnowballStemmer(\"english\")\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem(\"run\"))\n",
    "print(lem.lemmatize(\"running\",\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('title', 'NN'),\n",
       " ('post', 'NN'),\n",
       " ('trending', 'VBG'),\n",
       " ('quora', 'JJ'),\n",
       " ('question', 'NN'),\n",
       " ('popular', 'JJ'),\n",
       " ('responses', 'NNS'),\n",
       " ('boiling', 'VBG'),\n",
       " ('â€œitâ€™s', 'JJ'),\n",
       " ('unemployment', 'NN'),\n",
       " ('rate', 'NN'),\n",
       " ('3.6', 'CD')]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "nltk.pos_tag(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The title of this post was a trending Quora question, with popular responses boiling down to â€œitâ€™s not, the unemployment rate is 3.6%']"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "frases = nltk.sent_tokenize(texto)\n",
    "frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The\ntitle\nof\nthis\npost\nbe\na\ntrend\nQuora\nquestion\n,\nwith\npopular\nresponse\nboil\ndown\nto\nâ€œitâ€™s\nnot\n,\nthe\nunemployment\nrate\nbe\n3.6\n%\n"
     ]
    }
   ],
   "source": [
    "for frase in frases:\n",
    "    for palabra in nltk.word_tokenize(frase):\n",
    "        print(lem.lemmatize(palabra, get_wordnet_pos(palabra)))"
   ]
  },
  {
   "source": [
    "## Traduccion de variables textuales\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-3493d422c404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"esto es un curso de programacion\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# actual source language that will be recognized by Google Translator when the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[0;32m     80\u001b[0m                                     token=token, override=override)\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'var '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unicode-escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "print(translator.detect(\"esto es un curso de programacion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-b8d2b7b0210d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Esto es un curso de programacion\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[0;32m     80\u001b[0m                                     token=token, override=override)\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'var '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unicode-escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "print(translator.translate(\"Esto es un curso de programacion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-962e660f2ecd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpeliculas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"La comunidad del anillo\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Las dos torres\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"El retorno del rey\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                 \u001b[0mtranslated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[0;32m     80\u001b[0m                                     token=token, override=override)\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'var '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unicode-escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "peliculas = translator.translate([\"La comunidad del anillo\", \"Las dos torres\", \"El retorno del rey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pelicula in peliculas:\n",
    "    print(pelicula.origin, \"->\", pelicula.text)"
   ]
  },
  {
   "source": [
    "## Sentiment analysis\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "lista1 = [\"Bueno\", \"Malo\", \"Malo\"]\n",
    "lista2 = [\"Lo reomendaría mucho\",\n",
    "            \"Es el peor producto que he comprado nunca\",\n",
    "            \"Ni loco compraría ese producto\"]\n",
    "\n",
    "df = pd.DataFrame({\"Sentimiento\": lista1, \"Valoracion\": lista2})\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, ngram_range=(1,2),tokenizer=token.tokenize)\n",
    "text_counts=cv.fit_transform(df['Valoracion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 15)\t1\n  (0, 29)\t1\n  (0, 0)\t1\n  (0, 19)\t1\n  (0, 16)\t1\n  (0, 30)\t1\n  (0, 2)\t1\n  (1, 9)\t1\n  (1, 7)\t1\n  (1, 23)\t1\n  (1, 25)\t1\n  (1, 27)\t1\n  (1, 13)\t1\n  (1, 3)\t1\n  (1, 22)\t1\n  (1, 10)\t1\n  (1, 8)\t1\n  (1, 24)\t1\n  (1, 26)\t1\n  (1, 28)\t1\n  (1, 14)\t1\n  (1, 4)\t1\n  (2, 0)\t1\n  (2, 25)\t1\n  (2, 20)\t1\n  (2, 17)\t1\n  (2, 5)\t1\n  (2, 11)\t1\n  (2, 21)\t1\n  (2, 18)\t1\n  (2, 6)\t1\n  (2, 1)\t1\n  (2, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "print(text_counts)"
   ]
  },
  {
   "source": [
    "### Clasificacion automatica de texto"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['Sentimiento'], test_size=0.5, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultinomialNB Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf  = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy: \", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    Bueno\n",
       "2     Malo\n",
       "Name: Sentimiento, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Malo', 'Malo'], dtype='<U4')"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "source": [
    "## Topic Modeling\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "texto=open(path + \"/english_example.txt\", \"r\").read()\n",
    "frases = sent_tokenize(texto)\n",
    "frases = pd.DataFrame({\"Frases\": frases})\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preproceso(texto):\n",
    "    resultado = []\n",
    "    for token in gensim.utils.simple_preprocess(texto):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            resultado.append(stemmer.stem(token))\n",
    "        return resultado\n",
    "\n",
    "frasesprocesadas = frases[\"Frases\"].map(preproceso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0         []\n",
       "1    [hello]\n",
       "2     [love]\n",
       "Name: Frases, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "frasesprocesadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(frasesprocesadas)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in frasesprocesadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[], [], []]"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-94d1860aa6c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tema: {} \\npalabras: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"auto-tuning alpha not implemented in LdaMulticore; use plain LdaModel.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         super(LdaMulticore, self).__init__(\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python39\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=5, workers=3)\n",
    "for idx, topic in lda_model.print_topic(-1):\n",
    "    print('Tema: {} \\npalabras: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}